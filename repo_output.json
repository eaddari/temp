[
  {
    "file": "microservices/input_integration_pipeline/source/fetch_repo_from_blob/endpoints/api.py",
    "imports": [
      {
        "module": "fastapi",
        "type": "external"
      },
      {
        "module": "fastapi.responses",
        "type": "external"
      },
      {
        "module": "source.container_services",
        "type": "external"
      },
      {
        "module": "source.blob_services",
        "type": "external"
      },
      {
        "module": "source.schemas",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/input_integration_pipeline/source/fetch_repo_from_blob/main.py",
    "imports": [
      {
        "module": "uvicorn",
        "type": "external"
      },
      {
        "module": "fastapi",
        "type": "external"
      },
      {
        "module": "fastapi.middleware.cors",
        "type": "external"
      },
      {
        "module": "endpoints.api",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/input_integration_pipeline/source/fetch_repo_from_blob/source/schemas.py",
    "imports": [
      {
        "module": "pydantic",
        "type": "external"
      },
      {
        "module": "typing",
        "type": "external"
      }
    ],
    "classes": [
      {
        "name": "BlobRequest",
        "methods": [],
        "decorators": [],
        "inheritances": [
          "BaseModel"
        ]
      },
      {
        "name": "BlobToParquetRequest",
        "methods": [],
        "decorators": [],
        "inheritances": [
          "BaseModel"
        ]
      },
      {
        "name": "BlobDownloadResponse",
        "methods": [],
        "decorators": [],
        "inheritances": [
          "BaseModel"
        ]
      },
      {
        "name": "ContainerRequest",
        "methods": [],
        "decorators": [],
        "inheritances": [
          "BaseModel"
        ]
      },
      {
        "name": "ContainerDownloadRequest",
        "methods": [],
        "decorators": [],
        "inheritances": [
          "BaseModel"
        ]
      },
      {
        "name": "FilesByTypeRequest",
        "methods": [],
        "decorators": [],
        "inheritances": [
          "BaseModel"
        ]
      },
      {
        "name": "ContainerToParquetRequest",
        "methods": [],
        "decorators": [],
        "inheritances": [
          "BaseModel"
        ]
      },
      {
        "name": "ContainerAccessResponse",
        "methods": [],
        "decorators": [],
        "inheritances": [
          "BaseModel"
        ]
      },
      {
        "name": "ContainerDownloadResponse",
        "methods": [],
        "decorators": [],
        "inheritances": [
          "BaseModel"
        ]
      },
      {
        "name": "FilesByTypeResponse",
        "methods": [],
        "decorators": [],
        "inheritances": [
          "BaseModel"
        ]
      }
    ],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/input_integration_pipeline/source/fetch_repo_from_blob/tests/test_fetch_repo_from_blob.py",
    "imports": [
      {
        "module": "pytest",
        "type": "external"
      },
      {
        "module": "pytest_asyncio",
        "type": "external"
      },
      {
        "module": "unittest.mock",
        "type": "external"
      },
      {
        "module": "source.blob_services",
        "type": "external"
      },
      {
        "module": "source.container_services",
        "type": "external"
      },
      {
        "module": "unittest.mock",
        "type": "external"
      },
      {
        "module": "source",
        "type": "external"
      }
    ],
    "classes": [
      {
        "name": "AsyncBlobIterator",
        "methods": [
          {
            "name": "__aiter__",
            "content": "        def __aiter__(self):\n            async def gen():\n                blob_mock = MagicMock()\n                blob_mock.name = \"test_blob.txt\"\n                yield blob_mock\n            return gen()",
            "signature": {
              "args": [
                "self"
              ],
              "defaults": []
            },
            "decorators": []
          }
        ],
        "decorators": [],
        "inheritances": []
      }
    ],
    "functions": [
      {
        "name": "mock_blob_client",
        "content": "def mock_blob_client():\n    mock = AsyncMock()\n    mock.download_blob.return_value.readall.return_value = b\"test content\"\n    return mock",
        "signature": {
          "args": [],
          "defaults": []
        },
        "decorators": [
          "pytest_asyncio.fixture"
        ]
      },
      {
        "name": "mock_container_client_fixture",
        "content": "def mock_container_client_fixture():\n    from unittest.mock import MagicMock, AsyncMock\n\n    mock = MagicMock()\n    mock.get_container_properties = AsyncMock(return_value={})\n\n    class AsyncBlobIterator:\n        def __aiter__(self):\n            async def gen():\n                blob_mock = MagicMock()\n                blob_mock.name = \"test_blob.txt\"\n                yield blob_mock\n            return gen()\n\n    def list_blobs(*args, **kwargs):\n        return AsyncBlobIterator()\n    mock.list_blobs = list_blobs\n\n    def get_blob_client(name):\n        blob_client_mock = AsyncMock()\n        blob_client_mock.download_blob.return_value.readall.return_value = b\"test content\"\n        blob_client_mock.download_blob.return_value = AsyncMock()\n        blob_client_mock.download_blob.return_value.readall.return_value = b\"test content\"\n        return blob_client_mock\n    mock.get_blob_client.side_effect = get_blob_client\n\n    return mock",
        "signature": {
          "args": [],
          "defaults": []
        },
        "decorators": [
          "pytest_asyncio.fixture"
        ]
      },
      {
        "name": "list_blobs",
        "content": "    def list_blobs(*args, **kwargs):\n        return AsyncBlobIterator()",
        "signature": {
          "args": [],
          "defaults": []
        },
        "decorators": []
      },
      {
        "name": "get_blob_client",
        "content": "    def get_blob_client(name):\n        blob_client_mock = AsyncMock()\n        blob_client_mock.download_blob.return_value.readall.return_value = b\"test content\"\n        blob_client_mock.download_blob.return_value = AsyncMock()\n        blob_client_mock.download_blob.return_value.readall.return_value = b\"test content\"\n        return blob_client_mock",
        "signature": {
          "args": [
            "name"
          ],
          "defaults": []
        },
        "decorators": []
      },
      {
        "name": "patch_blob_service_client",
        "content": "def patch_blob_service_client(monkeypatch, mock_container_client_fixture):\n    from source import container_services, blob_services\n    monkeypatch.setattr(container_services, \"blob_service_client\", MagicMock(get_container_client=MagicMock(return_value=mock_container_client_fixture)))\n    monkeypatch.setattr(blob_services, \"blob_service_client\", MagicMock(get_container_client=MagicMock(return_value=mock_container_client_fixture)))",
        "signature": {
          "args": [
            "monkeypatch",
            "mock_container_client_fixture"
          ],
          "defaults": []
        },
        "decorators": [
          "pytest.fixture(autouse=True)"
        ]
      }
    ],
    "calls": [
      {
        "caller_function": "mock_blob_client",
        "caller_class": null,
        "called_function": "AsyncMock"
      },
      {
        "caller_function": "mock_container_client_fixture",
        "caller_class": null,
        "called_function": "MagicMock"
      },
      {
        "caller_function": "mock_container_client_fixture",
        "caller_class": null,
        "called_function": "AsyncMock"
      },
      {
        "caller_function": "__aiter__",
        "caller_class": "AsyncBlobIterator",
        "called_function": "MagicMock"
      },
      {
        "caller_function": "__aiter__",
        "caller_class": "AsyncBlobIterator",
        "called_function": "gen"
      },
      {
        "caller_function": "list_blobs",
        "caller_class": null,
        "called_function": "AsyncBlobIterator"
      },
      {
        "caller_function": "get_blob_client",
        "caller_class": null,
        "called_function": "AsyncMock"
      },
      {
        "caller_function": "get_blob_client",
        "caller_class": null,
        "called_function": "AsyncMock"
      },
      {
        "caller_function": "patch_blob_service_client",
        "caller_class": null,
        "called_function": "setattr"
      },
      {
        "caller_function": "patch_blob_service_client",
        "caller_class": null,
        "called_function": "MagicMock"
      },
      {
        "caller_function": "patch_blob_service_client",
        "caller_class": null,
        "called_function": "MagicMock"
      },
      {
        "caller_function": "patch_blob_service_client",
        "caller_class": null,
        "called_function": "setattr"
      },
      {
        "caller_function": "patch_blob_service_client",
        "caller_class": null,
        "called_function": "MagicMock"
      },
      {
        "caller_function": "patch_blob_service_client",
        "caller_class": null,
        "called_function": "MagicMock"
      },
      {
        "caller_function": "patch_blob_service_client",
        "caller_class": null,
        "called_function": "fixture"
      }
    ]
  },
  {
    "file": "microservices/input_integration_pipeline/source/fetch_repo_from_blob/utilities/chunk_rows.py",
    "imports": [
      {
        "module": "typing",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [
      {
        "name": "chunk_rows",
        "content": "def chunk_rows(rows: List[Any], chunk_size: int) -> Iterable[List[Any]]:\n    \"\"\"\n    Splitta il file che arriva in input in chunk di righe di dimensione massima chunk_size.\n    Args:\n        rows (list): lista di righe da splittare.\n        chunk_size (int): La dimensione massima di righe per chunk.\n    Yields:\n        list: Un chunk di righe.\n    \"\"\"\n    for i in range(0, len(rows), chunk_size):\n        yield rows[i:i + chunk_size]",
        "signature": {
          "args": [
            "rows",
            "chunk_size"
          ],
          "defaults": []
        },
        "decorators": []
      }
    ],
    "calls": [
      {
        "caller_function": "chunk_rows",
        "caller_class": null,
        "called_function": "range"
      },
      {
        "caller_function": "chunk_rows",
        "caller_class": null,
        "called_function": "len"
      }
    ]
  },
  {
    "file": "microservices/input_integration_pipeline/source/fetch_repo_from_blob/utilities/retry.py",
    "imports": [
      {
        "module": "tenacity",
        "type": "external"
      },
      {
        "module": "azure.core.exceptions",
        "type": "external"
      },
      {
        "module": "functools",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [
      {
        "name": "azure_retry",
        "content": "def azure_retry(function):\n    \"\"\"\n    Decoratore per gestire i retry delle chiamate Azure.\n    \"\"\"\n    @functools.wraps(function)\n    async def wrapper(*args, **kwargs):\n        async for attempt in AsyncRetrying(\n            stop=stop_after_attempt(5),\n            wait=wait_exponential(multiplier=1, min=2, max=10),\n            retry=retry_if_exception_type(AzureError)\n        ):\n            with attempt:\n                return await function(*args, **kwargs)\n    return wrapper",
        "signature": {
          "args": [
            "function"
          ],
          "defaults": []
        },
        "decorators": []
      }
    ],
    "calls": [
      {
        "caller_function": "azure_retry",
        "caller_class": null,
        "called_function": "AsyncRetrying"
      },
      {
        "caller_function": "azure_retry",
        "caller_class": null,
        "called_function": "stop_after_attempt"
      },
      {
        "caller_function": "azure_retry",
        "caller_class": null,
        "called_function": "wait_exponential"
      },
      {
        "caller_function": "azure_retry",
        "caller_class": null,
        "called_function": "retry_if_exception_type"
      },
      {
        "caller_function": "azure_retry",
        "caller_class": null,
        "called_function": "function"
      },
      {
        "caller_function": "azure_retry",
        "caller_class": null,
        "called_function": "wraps"
      }
    ]
  },
  {
    "file": "microservices/input_integration_pipeline/source/repo_downloader/endpoints/status.py",
    "imports": [
      {
        "module": "fastapi",
        "type": "external"
      },
      {
        "module": "typing",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [
      {
        "name": "get_status",
        "content": "def get_status() -> Dict:\n    \"\"\"\n    Check the health status of the API.\n\n    Returns:\n        Dict: A dictionary containing the status message\n    \"\"\"\n    return {\"message\": 200}",
        "signature": {
          "args": [],
          "defaults": []
        },
        "decorators": [
          "router.get('/status', summary='Check the status of the API.')"
        ]
      }
    ],
    "calls": [
      {
        "caller_function": "get_status",
        "caller_class": null,
        "called_function": "get"
      }
    ]
  },
  {
    "file": "microservices/input_integration_pipeline/source/repo_downloader/main.py",
    "imports": [
      {
        "module": "uvicorn",
        "type": "external"
      },
      {
        "module": "fastapi",
        "type": "external"
      },
      {
        "module": "fastapi.middleware.cors",
        "type": "external"
      },
      {
        "module": "endpoints.status",
        "type": "external"
      },
      {
        "module": "endpoints.repo_download_api",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/input_integration_pipeline/source/repo_downloader/source/__init__.py",
    "imports": [],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/input_integration_pipeline/source/repo_downloader/tests/__init__.py",
    "imports": [],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/input_integration_pipeline/source/upload_repo_on_blob/main.py",
    "imports": [
      {
        "module": "uvicorn",
        "type": "external"
      },
      {
        "module": "fastapi",
        "type": "external"
      },
      {
        "module": "fastapi.middleware.cors",
        "type": "external"
      },
      {
        "module": "src.endpoints.endpoint_uploader",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/input_integration_pipeline/source/upload_repo_on_blob/src/config/settings.py",
    "imports": [
      {
        "module": "os",
        "type": "external"
      },
      {
        "module": "dotenv",
        "type": "external"
      }
    ],
    "classes": [
      {
        "name": "Settings",
        "methods": [],
        "decorators": [],
        "inheritances": []
      }
    ],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/input_integration_pipeline/source/upload_repo_on_blob/src/endpoints/endpoint_uploader.py",
    "imports": [
      {
        "module": "fastapi",
        "type": "external"
      },
      {
        "module": "pydantic",
        "type": "external"
      },
      {
        "module": "typing",
        "type": "external"
      },
      {
        "module": "src.services.blob_manager",
        "type": "external"
      },
      {
        "module": "src.services.uploader",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/input_integration_pipeline/source/upload_repo_on_blob/src/services/uploader.py",
    "imports": [
      {
        "module": "os",
        "type": "external"
      },
      {
        "module": "typing",
        "type": "external"
      },
      {
        "module": "concurrent.futures",
        "type": "external"
      },
      {
        "module": "azure.storage.blob",
        "type": "external"
      },
      {
        "module": "pydantic",
        "type": "external"
      },
      {
        "module": "src.services.blob_manager",
        "type": "external"
      },
      {
        "module": "src.utils.helpers",
        "type": "external"
      }
    ],
    "classes": [
      {
        "name": "RepoUploader",
        "methods": [
          {
            "name": "__init__",
            "content": "    def __init__(self, blob_manager: BlobManager):\n        \"\"\"\n        Args:\n            blob_manager (BlobManager): Blob manager used to access Azure Storage.\n        \"\"\"\n        self.blob_manager = blob_manager",
            "signature": {
              "args": [
                "self",
                "blob_manager"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "upload_repository",
            "content": "    def upload_repository(self, repo_path: str, max_workers: int = 8) -> Tuple[str, bool]:\n        \"\"\"\n        Uploads files from a local repo to Azure, syncing the container.\n\n        Args:\n            repo_path (str): Path to the local repository.\n            max_workers (int): Number of threads for parallel upload.\n\n        Returns:\n            Tuple[str, bool]: Container name and True if created, else False.\n        \"\"\"\n        container_name = sanitize_container_name(os.path.basename(os.path.normpath(repo_path)))\n        container_client, created = self.blob_manager.ensure_container_exists(container_name)\n        files: List[Tuple[str, str]] = []\n        for root, _, filenames in os.walk(repo_path):\n            for filename in filenames:\n                full_path = os.path.join(root, filename)\n                relative_path = os.path.relpath(full_path, repo_path).replace(\"\\\\\", \"/\")\n                if not is_path_allowed(relative_path):\n                    continue\n                files.append((full_path, relative_path))\n        existing_blobs = self.blob_manager.get_blob_name_set(container_name)\n        local_paths = {rel for _, rel in files}\n        to_delete = existing_blobs - local_paths\n        for blob_name in to_delete:\n            container_client.delete_blob(blob_name)\n        self._speed_up_upload(container_client, files, max_workers)\n        return container_name, created",
            "signature": {
              "args": [
                "self",
                "repo_path",
                "max_workers"
              ],
              "defaults": [
                "8"
              ]
            },
            "decorators": []
          },
          {
            "name": "_speed_up_upload",
            "content": "    def _speed_up_upload(self, container_client: ContainerClient, files: List[Tuple[str, str]], max_workers: int):\n        \"\"\"\n        Uploads files in parallel using threads.\n\n        Args:\n            container_client (ContainerClient): Target container.\n            files (List[Tuple[str, str]]): List of (full_path, relative_path).\n            max_workers (int): Number of threads.\n        \"\"\"\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = [\n                executor.submit(self.upload_single_file, container_client, full, rel)\n                for full, rel in files\n            ]\n            for future in as_completed(futures):\n                future.result()",
            "signature": {
              "args": [
                "self",
                "container_client",
                "files",
                "max_workers"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "upload_single_file",
            "content": "    def upload_single_file(self, container_client: ContainerClient, full_path: str, relative_path: str):\n        \"\"\"\n        Uploads a single file to Azure Blob Storage.\n\n        Args:\n            container_client (ContainerClient): Target container.\n            full_path (str): Absolute path to the file.\n            relative_path (str): Relative path used as blob name.\n        \"\"\"\n        blob_client = container_client.get_blob_client(relative_path)\n        with open(full_path, \"rb\") as data:\n            blob_client.upload_blob(data, overwrite=True)",
            "signature": {
              "args": [
                "self",
                "container_client",
                "full_path",
                "relative_path"
              ],
              "defaults": []
            },
            "decorators": []
          }
        ],
        "decorators": [],
        "inheritances": []
      }
    ],
    "functions": [],
    "calls": [
      {
        "caller_function": "upload_repository",
        "caller_class": "RepoUploader",
        "called_function": "sanitize_container_name"
      },
      {
        "caller_function": "upload_repository",
        "caller_class": "RepoUploader",
        "called_function": "basename"
      },
      {
        "caller_function": "upload_repository",
        "caller_class": "RepoUploader",
        "called_function": "normpath"
      },
      {
        "caller_function": "upload_repository",
        "caller_class": "RepoUploader",
        "called_function": "ensure_container_exists"
      },
      {
        "caller_function": "upload_repository",
        "caller_class": "RepoUploader",
        "called_function": "walk"
      },
      {
        "caller_function": "upload_repository",
        "caller_class": "RepoUploader",
        "called_function": "join"
      },
      {
        "caller_function": "upload_repository",
        "caller_class": "RepoUploader",
        "called_function": "replace"
      },
      {
        "caller_function": "upload_repository",
        "caller_class": "RepoUploader",
        "called_function": "relpath"
      },
      {
        "caller_function": "upload_repository",
        "caller_class": "RepoUploader",
        "called_function": "is_path_allowed"
      },
      {
        "caller_function": "upload_repository",
        "caller_class": "RepoUploader",
        "called_function": "append"
      },
      {
        "caller_function": "upload_repository",
        "caller_class": "RepoUploader",
        "called_function": "get_blob_name_set"
      },
      {
        "caller_function": "upload_repository",
        "caller_class": "RepoUploader",
        "called_function": "delete_blob"
      },
      {
        "caller_function": "upload_repository",
        "caller_class": "RepoUploader",
        "called_function": "_speed_up_upload"
      },
      {
        "caller_function": "_speed_up_upload",
        "caller_class": "RepoUploader",
        "called_function": "ThreadPoolExecutor"
      },
      {
        "caller_function": "_speed_up_upload",
        "caller_class": "RepoUploader",
        "called_function": "submit"
      },
      {
        "caller_function": "_speed_up_upload",
        "caller_class": "RepoUploader",
        "called_function": "as_completed"
      },
      {
        "caller_function": "_speed_up_upload",
        "caller_class": "RepoUploader",
        "called_function": "result"
      },
      {
        "caller_function": "upload_single_file",
        "caller_class": "RepoUploader",
        "called_function": "get_blob_client"
      },
      {
        "caller_function": "upload_single_file",
        "caller_class": "RepoUploader",
        "called_function": "open"
      },
      {
        "caller_function": "upload_single_file",
        "caller_class": "RepoUploader",
        "called_function": "upload_blob"
      }
    ]
  },
  {
    "file": "microservices/input_integration_pipeline/source/upload_repo_on_blob/src/utils/helpers.py",
    "imports": [
      {
        "module": "re",
        "type": "external"
      },
      {
        "module": "os",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [
      {
        "name": "sanitize_container_name",
        "content": "def sanitize_container_name(name: str) -> str:\n    \"\"\"\n    Sanitizes a string to produce a valid Azure Blob Storage container name.\n\n    Rules enforced:\n    - Must be between 3 and 63 characters long.\n    - Must contain only lowercase letters, numbers, and hyphens.\n    - Must begin and end with a letter or a number.\n    - Hyphens must be surrounded by letters or numbers (no leading/trailing hyphens, no consecutive hyphens).\n\n    If the cleaned name does not meet these criteria, a fallback UUID-based name is generated.\n\n    Args:\n        name (str): The input string, typically a folder or repository name.\n\n    Returns:\n        str: A valid Azure container name.\n    \"\"\"\n    name = re.sub(r'[^a-z0-9-]', '-', name.lower())\n    name = re.sub(r'-{2,}', '-', name)\n    name = name.strip('-')\n    name = name[:63].strip('-')  \n\n    return name ",
        "signature": {
          "args": [
            "name"
          ],
          "defaults": []
        },
        "decorators": []
      },
      {
        "name": "is_path_allowed",
        "content": "def is_path_allowed(relative_path: str) -> bool:\n    \"\"\"\n    Checks if a file path is allowed for upload.\n\n    Args:\n        relative_path (str): File path relative to the repository root.\n\n    Returns:\n        bool: True if allowed, False otherwise.\n    \"\"\"\n    parts = relative_path.split(\"/\")\n    if any(part in EXCLUDED_DIRS for part in parts):\n        return False\n    _, ext = os.path.splitext(relative_path)\n    if ext.lower() not in ALLOWED_EXTENSIONS:\n        return False\n    return True",
        "signature": {
          "args": [
            "relative_path"
          ],
          "defaults": []
        },
        "decorators": []
      }
    ],
    "calls": [
      {
        "caller_function": "sanitize_container_name",
        "caller_class": null,
        "called_function": "sub"
      },
      {
        "caller_function": "sanitize_container_name",
        "caller_class": null,
        "called_function": "lower"
      },
      {
        "caller_function": "sanitize_container_name",
        "caller_class": null,
        "called_function": "sub"
      },
      {
        "caller_function": "sanitize_container_name",
        "caller_class": null,
        "called_function": "strip"
      },
      {
        "caller_function": "sanitize_container_name",
        "caller_class": null,
        "called_function": "strip"
      },
      {
        "caller_function": "is_path_allowed",
        "caller_class": null,
        "called_function": "split"
      },
      {
        "caller_function": "is_path_allowed",
        "caller_class": null,
        "called_function": "any"
      },
      {
        "caller_function": "is_path_allowed",
        "caller_class": null,
        "called_function": "splitext"
      },
      {
        "caller_function": "is_path_allowed",
        "caller_class": null,
        "called_function": "lower"
      }
    ]
  },
  {
    "file": "microservices/input_integration_pipeline/source/upload_repo_on_blob/tests/__init__.py",
    "imports": [],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/input_integration_pipeline/source/upload_repo_on_blob/tests/test_endpoints.py",
    "imports": [
      {
        "module": "pytest",
        "type": "external"
      },
      {
        "module": "fastapi.testclient",
        "type": "external"
      },
      {
        "module": "unittest.mock",
        "type": "external"
      },
      {
        "module": "main",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [
      {
        "name": "test_check_connection_success",
        "content": "def test_check_connection_success(mock_blob_manager):\n    \"\"\"\n    Verifica che la connessione vada a buon fine.\n    \"\"\"\n    response = client.get(\"/v2/repo/check_connection\")\n    assert response.status_code == 200\n    body = response.json()\n    assert body[\"status\"] == \"success\"\n    assert \"Connection to Azure Blob Storage is successful\" in body[\"message\"]",
        "signature": {
          "args": [
            "mock_blob_manager"
          ],
          "defaults": []
        },
        "decorators": [
          "patch('src.endpoints.endpoint_uploader.BlobManager')"
        ]
      },
      {
        "name": "test_check_connection_failure",
        "content": "def test_check_connection_failure(mock_blob_manager):\n    \"\"\"\n    Verifica il comportamento in caso di errore durante la connessione.\n    \"\"\"\n    response = client.get(\"/v2/repo/check_connection\")\n    assert response.status_code == 200\n    body = response.json()\n    assert body[\"status\"] == \"error\"\n    assert body[\"message\"] == \"Failed to connect to Azure Blob Storage.\"\n    assert \"Connection failed\" in body[\"error\"]",
        "signature": {
          "args": [
            "mock_blob_manager"
          ],
          "defaults": []
        },
        "decorators": [
          "patch('src.endpoints.endpoint_uploader.BlobManager', side_effect=Exception('Connection failed'))"
        ]
      },
      {
        "name": "test_upload_repo_success",
        "content": "def test_upload_repo_success(mock_blob_manager, mock_repo_uploader):\n    \"\"\"\n    Verifica che l'upload della repository vada a buon fine.\n    \"\"\"\n    mock_instance = mock_repo_uploader.return_value\n    mock_instance.upload_repository.return_value = (\"mock-container\", True)\n\n    response = client.post(\"/v2/repo/upload\", params={\"repo_path\": \"/mock/path\"})\n    assert response.status_code == 200\n    body = response.json()\n    assert body[\"status\"] == \"success\"\n    assert body[\"container_name\"] == \"mock-container\"\n    assert \"created\" in body[\"message\"]",
        "signature": {
          "args": [
            "mock_blob_manager",
            "mock_repo_uploader"
          ],
          "defaults": []
        },
        "decorators": [
          "patch('src.endpoints.endpoint_uploader.RepoUploader')",
          "patch('src.endpoints.endpoint_uploader.BlobManager')"
        ]
      },
      {
        "name": "test_upload_repo_failure",
        "content": "def test_upload_repo_failure(mock_blob_manager, mock_repo_uploader):\n    \"\"\"\n    Verifica il comportamento in caso di errore durante l'upload.\n    \"\"\"\n    response = client.post(\"/v2/repo/upload\", params={\"repo_path\": \"/mock/path\"})\n    assert response.status_code == 200\n    body = response.json()\n    assert body[\"status\"] == \"error\"\n    assert \"Upload failed\" in body[\"error\"]",
        "signature": {
          "args": [
            "mock_blob_manager",
            "mock_repo_uploader"
          ],
          "defaults": []
        },
        "decorators": [
          "patch('src.endpoints.endpoint_uploader.RepoUploader', side_effect=Exception('Upload failed'))",
          "patch('src.endpoints.endpoint_uploader.BlobManager')"
        ]
      }
    ],
    "calls": [
      {
        "caller_function": "test_check_connection_success",
        "caller_class": null,
        "called_function": "get"
      },
      {
        "caller_function": "test_check_connection_success",
        "caller_class": null,
        "called_function": "json"
      },
      {
        "caller_function": "test_check_connection_success",
        "caller_class": null,
        "called_function": "patch"
      },
      {
        "caller_function": "test_check_connection_failure",
        "caller_class": null,
        "called_function": "get"
      },
      {
        "caller_function": "test_check_connection_failure",
        "caller_class": null,
        "called_function": "json"
      },
      {
        "caller_function": "test_check_connection_failure",
        "caller_class": null,
        "called_function": "patch"
      },
      {
        "caller_function": "test_check_connection_failure",
        "caller_class": null,
        "called_function": "Exception"
      },
      {
        "caller_function": "test_upload_repo_success",
        "caller_class": null,
        "called_function": "post"
      },
      {
        "caller_function": "test_upload_repo_success",
        "caller_class": null,
        "called_function": "json"
      },
      {
        "caller_function": "test_upload_repo_success",
        "caller_class": null,
        "called_function": "patch"
      },
      {
        "caller_function": "test_upload_repo_success",
        "caller_class": null,
        "called_function": "patch"
      },
      {
        "caller_function": "test_upload_repo_failure",
        "caller_class": null,
        "called_function": "post"
      },
      {
        "caller_function": "test_upload_repo_failure",
        "caller_class": null,
        "called_function": "json"
      },
      {
        "caller_function": "test_upload_repo_failure",
        "caller_class": null,
        "called_function": "patch"
      },
      {
        "caller_function": "test_upload_repo_failure",
        "caller_class": null,
        "called_function": "Exception"
      },
      {
        "caller_function": "test_upload_repo_failure",
        "caller_class": null,
        "called_function": "patch"
      }
    ]
  },
  {
    "file": "microservices/input_integration_pipeline/tests/test_workflow.py",
    "imports": [
      {
        "module": "sys",
        "type": "external_builtin"
      },
      {
        "module": "os",
        "type": "external"
      },
      {
        "module": "pytest",
        "type": "external"
      },
      {
        "module": "requests",
        "type": "external"
      },
      {
        "module": "main",
        "type": "external"
      },
      {
        "module": "source",
        "type": "external"
      },
      {
        "module": "typing",
        "type": "external"
      },
      {
        "module": "types",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [
      {
        "name": "mock_get",
        "content": "def mock_get(url: str):\n    return SimpleNamespace(status_code=200)",
        "signature": {
          "args": [
            "url"
          ],
          "defaults": []
        },
        "decorators": []
      },
      {
        "name": "mock_post_success",
        "content": "def mock_post_success(url: str, json: dict):\n    return SimpleNamespace(status_code=200, json=lambda: {\"path\": \"/mock/path\"})",
        "signature": {
          "args": [
            "url",
            "json"
          ],
          "defaults": []
        },
        "decorators": []
      },
      {
        "name": "mock_post_failure",
        "content": "def mock_post_failure(url: str, json: dict):\n    return SimpleNamespace(status_code=404, text=\"Not found\", json=lambda: {})",
        "signature": {
          "args": [
            "url",
            "json"
          ],
          "defaults": []
        },
        "decorators": []
      },
      {
        "name": "mock_post_workflow",
        "content": "def mock_post_workflow(url: str, json: Optional[dict] = None, params: Optional[dict] = None):\n    if \"download_repository\" in url:\n        resp = SimpleNamespace(status_code=200, json=lambda: {\"path\": \"/mock/path\"})\n    elif \"repo/upload\" in url:\n        resp = SimpleNamespace(status_code=200, json=lambda: {\"container_name\": \"mock_container\"})\n    elif \"transform-container-to-parquet\" in url:\n        resp = SimpleNamespace(status_code=200, json=lambda: {\"message\": \"Transformation complete\"})\n    else:\n        resp = SimpleNamespace(status_code=200, json=lambda: {})\n    mock_post_workflow.calls.append(url)\n    return resp",
        "signature": {
          "args": [
            "url",
            "json",
            "params"
          ],
          "defaults": [
            "None",
            "None"
          ]
        },
        "decorators": []
      },
      {
        "name": "test_get_repo_status",
        "content": "def test_get_repo_status(monkeypatch: Any) -> None:\n    \"\"\"\n    Test get_repo_status to ensure it returns the correct status code.\n    Args:\n        monkeypatch (Any): Pytest fixture for patching functions.\n    Returns:\n        None\n    \"\"\"\n    monkeypatch.setattr(requests, \"get\", mock_get)\n    status = apicalls.get_repo_status(MOCK_ENDPOINT)\n    assert status == 200",
        "signature": {
          "args": [
            "monkeypatch"
          ],
          "defaults": []
        },
        "decorators": []
      },
      {
        "name": "test_get_repo_url_success",
        "content": "def test_get_repo_url_success(monkeypatch: Any) -> None:\n    \"\"\"\n    Test get_repo_url for a successful response.\n    Args:\n        monkeypatch (Any): Pytest fixture for patching functions.\n    Returns:\n        None\n    \"\"\"\n    monkeypatch.setattr(requests, \"post\", mock_post_success)\n    result = apicalls.get_repo_url(MOCK_ENDPOINT, {\"repository\": \"repo\"})\n    assert result == \"/mock/path\"",
        "signature": {
          "args": [
            "monkeypatch"
          ],
          "defaults": []
        },
        "decorators": []
      },
      {
        "name": "test_get_repo_url_failure",
        "content": "def test_get_repo_url_failure(monkeypatch: Any) -> None:\n    \"\"\"\n    Test get_repo_url for a failed response (status code != 200).\n    Args:\n        monkeypatch (Any): Pytest fixture for patching functions.\n    Returns:\n        None\n    \"\"\"\n    monkeypatch.setattr(requests, \"post\", mock_post_failure)\n    result = apicalls.get_repo_url(MOCK_ENDPOINT, {\"repository\": \"repo\"})\n    assert result is None",
        "signature": {
          "args": [
            "monkeypatch"
          ],
          "defaults": []
        },
        "decorators": []
      },
      {
        "name": "test_main_workflow",
        "content": "def test_main_workflow(monkeypatch: Any) -> None:\n    \"\"\"\n    Test the main workflow by mocking requests.post to simulate downstream services.\n    Args:\n        monkeypatch (Any): Pytest fixture for patching functions.\n    Returns:\n        None\n    \"\"\"\n    mock_post_workflow.calls = []\n    monkeypatch.setattr(requests, \"post\", mock_post_workflow)\n    main.__name__ = \"__main__\"\n    try:\n        exec(open(\"main.py\").read(), main.__dict__)\n    except Exception as e:\n        pytest.fail(f\"Workflow failed: {e}\")\n    assert len(mock_post_workflow.calls) == 3",
        "signature": {
          "args": [
            "monkeypatch"
          ],
          "defaults": []
        },
        "decorators": []
      }
    ],
    "calls": [
      {
        "caller_function": "mock_get",
        "caller_class": null,
        "called_function": "SimpleNamespace"
      },
      {
        "caller_function": "mock_post_success",
        "caller_class": null,
        "called_function": "SimpleNamespace"
      },
      {
        "caller_function": "mock_post_failure",
        "caller_class": null,
        "called_function": "SimpleNamespace"
      },
      {
        "caller_function": "mock_post_workflow",
        "caller_class": null,
        "called_function": "SimpleNamespace"
      },
      {
        "caller_function": "mock_post_workflow",
        "caller_class": null,
        "called_function": "SimpleNamespace"
      },
      {
        "caller_function": "mock_post_workflow",
        "caller_class": null,
        "called_function": "SimpleNamespace"
      },
      {
        "caller_function": "mock_post_workflow",
        "caller_class": null,
        "called_function": "SimpleNamespace"
      },
      {
        "caller_function": "mock_post_workflow",
        "caller_class": null,
        "called_function": "append"
      },
      {
        "caller_function": "test_get_repo_status",
        "caller_class": null,
        "called_function": "setattr"
      },
      {
        "caller_function": "test_get_repo_status",
        "caller_class": null,
        "called_function": "get_repo_status"
      },
      {
        "caller_function": "test_get_repo_url_success",
        "caller_class": null,
        "called_function": "setattr"
      },
      {
        "caller_function": "test_get_repo_url_success",
        "caller_class": null,
        "called_function": "get_repo_url"
      },
      {
        "caller_function": "test_get_repo_url_failure",
        "caller_class": null,
        "called_function": "setattr"
      },
      {
        "caller_function": "test_get_repo_url_failure",
        "caller_class": null,
        "called_function": "get_repo_url"
      },
      {
        "caller_function": "test_main_workflow",
        "caller_class": null,
        "called_function": "setattr"
      },
      {
        "caller_function": "test_main_workflow",
        "caller_class": null,
        "called_function": "exec"
      },
      {
        "caller_function": "test_main_workflow",
        "caller_class": null,
        "called_function": "read"
      },
      {
        "caller_function": "test_main_workflow",
        "caller_class": null,
        "called_function": "open"
      },
      {
        "caller_function": "test_main_workflow",
        "caller_class": null,
        "called_function": "fail"
      },
      {
        "caller_function": "test_main_workflow",
        "caller_class": null,
        "called_function": "len"
      }
    ]
  },
  {
    "file": "microservices/transforming_pipeline/components/anonimization/endpoints/anonimizator_api.py",
    "imports": [],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/components/anonimization/source/__init__.py",
    "imports": [],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/components/anonimization/tests/test_anonimizator_azure.py",
    "imports": [],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/components/cleaning_and_transformation/README.py",
    "imports": [],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/components/cleaning_and_transformation/endpoints/__init__.py",
    "imports": [],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/components/cleaning_and_transformation/endpoints/api.py",
    "imports": [
      {
        "module": "fastapi",
        "type": "external"
      },
      {
        "module": "fastapi.responses",
        "type": "external"
      },
      {
        "module": "source.text_cleaning_services",
        "type": "external"
      },
      {
        "module": "source.schemas",
        "type": "external"
      },
      {
        "module": "os",
        "type": "external"
      },
      {
        "module": "tempfile",
        "type": "external"
      },
      {
        "module": "datetime",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/components/cleaning_and_transformation/main.py",
    "imports": [
      {
        "module": "uvicorn",
        "type": "external"
      },
      {
        "module": "fastapi",
        "type": "external"
      },
      {
        "module": "fastapi.middleware.cors",
        "type": "external"
      },
      {
        "module": "endpoints.api",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/components/cleaning_and_transformation/source/schemas.py",
    "imports": [
      {
        "module": "pydantic",
        "type": "external"
      },
      {
        "module": "typing",
        "type": "external"
      },
      {
        "module": "datetime",
        "type": "external"
      }
    ],
    "classes": [
      {
        "name": "HealthResponse",
        "methods": [],
        "decorators": [],
        "inheritances": [
          "BaseModel"
        ]
      },
      {
        "name": "CleanParquetResponse",
        "methods": [],
        "decorators": [],
        "inheritances": [
          "BaseModel"
        ]
      },
      {
        "name": "ValidationResponse",
        "methods": [],
        "decorators": [],
        "inheritances": [
          "BaseModel"
        ]
      },
      {
        "name": "ErrorResponse",
        "methods": [],
        "decorators": [],
        "inheritances": [
          "BaseModel"
        ]
      },
      {
        "name": "Config",
        "methods": [],
        "decorators": [],
        "inheritances": []
      }
    ],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/components/cleaning_and_transformation/source/text_cleaner.py",
    "imports": [
      {
        "module": "pandas",
        "type": "external"
      },
      {
        "module": "typing",
        "type": "external"
      },
      {
        "module": "pydantic",
        "type": "external"
      },
      {
        "module": "re",
        "type": "external"
      },
      {
        "module": "pathlib",
        "type": "external"
      }
    ],
    "classes": [
      {
        "name": "TextCleaner",
        "methods": [
          {
            "name": "__init__",
            "content": "    def __init__(self, df: pd.DataFrame):\n        \"\"\"\n        Initialize the class with the DataFrame to extract\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            DataFrame containing the data to extract\n        \"\"\"\n        self.df = df",
            "signature": {
              "args": [
                "self",
                "df"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "__remove_extra_whitespace",
            "content": "    def __remove_extra_whitespace(self, text: str) -> str:\n        \"\"\"\n        Remove multiple spaces, tabs and excessive newlines\n        \n        Parameters\n        ----------\n        text : str\n            Text to clean\n            \n        Returns\n        -------\n        str\n            Text with normalized whitespace\n        \"\"\"\n        # Replace tabs with spaces\n        text = text.replace('\\t', ' ')\n        # Remove multiple spaces\n        text = re.sub(r' +', ' ', text)\n        # Remove multiple newlines with single one\n        text = re.sub(r'\\n+', '\\n', text)\n        # Remove spaces before and after newlines\n        text = re.sub(r' *\\n *', '\\n', text)\n        return text.strip()",
            "signature": {
              "args": [
                "self",
                "text"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "__remove_special_characters",
            "content": "    def __remove_special_characters(self, text: str) -> str:\n        \"\"\"\n        Remove unnecessary special characters from text\n        \n        Parameters\n        ----------\n        text : str\n            Text to clean\n            \n        Returns\n        -------\n        str\n            Text with special characters removed\n        \"\"\"\n        # Keep only letters, numbers, spaces, basic punctuation and HTML tags\n        text = re.sub(r'[^\\w\\s\\.\\,\\;\\:\\!\\?\\-\\(\\)\\[\\]\\{\\}\\\"\\'\\/\\n<>]', ' ', text)\n        return text",
            "signature": {
              "args": [
                "self",
                "text"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "__normalize_punctuation",
            "content": "    def __normalize_punctuation(self, text: str) -> str:\n        \"\"\"\n        Normalize punctuation spacing\n        \n        Parameters\n        ----------\n        text : str\n            Text to normalize\n            \n        Returns\n        -------\n        str\n            Text with normalized punctuation\n        \"\"\"\n        # Add space after punctuation if missing\n        text = re.sub(r'([.!?,;:])([A-Za-z])', r'\\1 \\2', text)\n        # Remove spaces before punctuation\n        text = re.sub(r'\\s+([.!?,;:])', r'\\1', text)\n        return text",
            "signature": {
              "args": [
                "self",
                "text"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "__remove_urls",
            "content": "    def __remove_urls(self, text: str) -> str:\n        \"\"\"\n        Remove URLs from text\n        \n        Parameters\n        ----------\n        text : str\n            Text containing URLs\n            \n        Returns\n        -------\n        str\n            Text with URLs removed\n        \"\"\"\n        # Remove http/https URLs\n        text = re.sub(r'https?://\\S+', '', text)\n        # Remove www URLs\n        text = re.sub(r'www\\.\\S+', '', text)\n        return text",
            "signature": {
              "args": [
                "self",
                "text"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "__remove_emails",
            "content": "    def __remove_emails(self, text: str) -> str:\n        \"\"\"\n        Remove email addresses\n        \n        Parameters\n        ----------\n        text : str\n            Text containing email addresses\n            \n        Returns\n        -------\n        str\n            Text with email addresses removed\n        \"\"\"\n        text = re.sub(r'\\S+@\\S+', '', text)\n        return text",
            "signature": {
              "args": [
                "self",
                "text"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "__fix_encoding_issues",
            "content": "    def __fix_encoding_issues(self, text: str) -> str:\n        \"\"\"\n        Fix common encoding issues\n        \n        Parameters\n        ----------\n        text : str\n            Text with potential encoding issues\n            \n        Returns\n        -------\n        str\n            Text with encoding issues fixed\n        \"\"\"\n        # Replace common malformed encoding characters\n        replacements = {\n            '\u00e2\u20ac\u2122': \"'\",\n            '\u00e2\u20ac\u0153': '\"',\n            '\u00e2\u20ac': '\"',\n            '\u00e2\u20ac\"': '-',\n            '\u00e2\u20ac\"': '\u2014',\n            '\u00c3\u00a9': '\u00e9',\n            '\u00c3\u00a8': '\u00e8',\n            '\u00c3 ': '\u00e0',\n            '\u00c3\u00b9': '\u00f9',\n            '\u00c3\u00b2': '\u00f2',\n        }\n        for old, new in replacements.items():\n            text = text.replace(old, new)\n        return text",
            "signature": {
              "args": [
                "self",
                "text"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "__lowercase_text",
            "content": "    def __lowercase_text(self, text: str) -> str:\n        \"\"\"\n        Convert text to lowercase\n        \n        Parameters\n        ----------\n        text : str\n            Text to convert\n            \n        Returns\n        -------\n        str\n            Lowercase text\n        \"\"\"\n        return text.lower()",
            "signature": {
              "args": [
                "self",
                "text"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "__remove_short_lines",
            "content": "    def __remove_short_lines(self, text: str) -> str:\n        \"\"\"\n        Remove lines that are too short (less than 3 characters)\n        \n        Parameters\n        ----------\n        text : str\n            Text with multiple lines\n            \n        Returns\n        -------\n        str\n            Text with short lines removed\n        \"\"\"\n        lines = text.split('\\n')\n        lines = [line for line in lines if len(line.strip()) > 2]\n        return '\\n'.join(lines)",
            "signature": {
              "args": [
                "self",
                "text"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "clean_text",
            "content": "    def clean_text(self, text: str) -> str:\n        \"\"\"\n        Main method that calls all cleaning methods in sequence\n        \n        Parameters\n        ----------\n        text : str\n            Text to clean\n            \n        Returns\n        -------\n        str\n            Cleaned text ready for chunking and embedding\n        \"\"\"\n        text = self.__fix_encoding_issues(text)\n        text = self.__remove_urls(text)\n        text = self.__remove_emails(text)\n        text = self.__remove_special_characters(text)\n        text = self.__normalize_punctuation(text)\n        text = self.__remove_extra_whitespace(text)\n        text = self.__lowercase_text(text)\n        text = self.__remove_short_lines(text)\n        return text",
            "signature": {
              "args": [
                "self",
                "text"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "get_txt_files",
            "content": "    def get_txt_files(self) -> Dict[str, str]:\n        \"\"\"\n        Identify files with .txt extension and return a dictionary\n        with filename and content\n        \n        Returns\n        -------\n        Dict[str, str]\n            Dictionary with key=filename, value=content\n        \"\"\"\n        if self.df is None or self.df.empty:\n            return {}\n        \n        # Assume the first column contains filenames and the second contains content\n        file_col = self.df.iloc[:, 0]\n        content_col = self.df.iloc[:, 1]\n        \n        txt_files = {}\n        \n        for idx, filename in enumerate(file_col):\n            # Check if file has .txt extension and is not requirements.txt\n            if str(filename).lower().endswith('.txt') and not str(filename).lower().endswith('requirements.txt'):\n                raw_content = str(content_col.iloc[idx])\n                cleaned_content = self.clean_text(raw_content)\n                txt_files[str(filename)] = cleaned_content\n        \n        return txt_files",
            "signature": {
              "args": [
                "self"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "get_txt_files_as_list",
            "content": "    def get_txt_files_as_list(self) -> List[tuple]:\n        \"\"\"\n        Return a list of tuples (filename, content) for .txt files\n        \n        Returns\n        -------\n        List[tuple]\n            List of tuples with (filename, content)\n        \"\"\"\n        txt_dict = self.get_txt_files()\n        return [(name, content) for name, content in txt_dict.items()]",
            "signature": {
              "args": [
                "self"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "save_cleaned_df",
            "content": "    def save_cleaned_df(self) -> None:\n        \"\"\"\n        Save updated DataFrame with cleaned content to a new parquet file\n        \n        Parameters\n        ----------\n        output_path : str\n            Path where to save the output parquet file\n        \"\"\"\n        if self.df is None or self.df.empty:\n            raise ValueError(\"No data to save\")\n        \n        # Create a copy of the dataframe\n        df_cleaned = self.df.copy()\n        \n        # Clean content for .txt files directly in second column\n        for idx in range(len(df_cleaned)):\n            filename = str(df_cleaned.iloc[idx, 0])\n            if filename.lower().endswith('.txt') and not filename.lower().split(\"/\")[-1] == 'requirements.txt':\n                raw_content = str(df_cleaned.iloc[idx, 1])\n                cleaned = self.clean_text(raw_content)\n                df_cleaned.iloc[idx, 1] = cleaned\n        \n        # Save to parquet\n        return df_cleaned",
            "signature": {
              "args": [
                "self"
              ],
              "defaults": []
            },
            "decorators": []
          }
        ],
        "decorators": [],
        "inheritances": []
      }
    ],
    "functions": [],
    "calls": [
      {
        "caller_function": "__remove_extra_whitespace",
        "caller_class": "TextCleaner",
        "called_function": "replace"
      },
      {
        "caller_function": "__remove_extra_whitespace",
        "caller_class": "TextCleaner",
        "called_function": "sub"
      },
      {
        "caller_function": "__remove_extra_whitespace",
        "caller_class": "TextCleaner",
        "called_function": "sub"
      },
      {
        "caller_function": "__remove_extra_whitespace",
        "caller_class": "TextCleaner",
        "called_function": "sub"
      },
      {
        "caller_function": "__remove_extra_whitespace",
        "caller_class": "TextCleaner",
        "called_function": "strip"
      },
      {
        "caller_function": "__remove_special_characters",
        "caller_class": "TextCleaner",
        "called_function": "sub"
      },
      {
        "caller_function": "__normalize_punctuation",
        "caller_class": "TextCleaner",
        "called_function": "sub"
      },
      {
        "caller_function": "__normalize_punctuation",
        "caller_class": "TextCleaner",
        "called_function": "sub"
      },
      {
        "caller_function": "__remove_urls",
        "caller_class": "TextCleaner",
        "called_function": "sub"
      },
      {
        "caller_function": "__remove_urls",
        "caller_class": "TextCleaner",
        "called_function": "sub"
      },
      {
        "caller_function": "__remove_emails",
        "caller_class": "TextCleaner",
        "called_function": "sub"
      },
      {
        "caller_function": "__fix_encoding_issues",
        "caller_class": "TextCleaner",
        "called_function": "items"
      },
      {
        "caller_function": "__fix_encoding_issues",
        "caller_class": "TextCleaner",
        "called_function": "replace"
      },
      {
        "caller_function": "__lowercase_text",
        "caller_class": "TextCleaner",
        "called_function": "lower"
      },
      {
        "caller_function": "__remove_short_lines",
        "caller_class": "TextCleaner",
        "called_function": "split"
      },
      {
        "caller_function": "__remove_short_lines",
        "caller_class": "TextCleaner",
        "called_function": "len"
      },
      {
        "caller_function": "__remove_short_lines",
        "caller_class": "TextCleaner",
        "called_function": "strip"
      },
      {
        "caller_function": "__remove_short_lines",
        "caller_class": "TextCleaner",
        "called_function": "join"
      },
      {
        "caller_function": "clean_text",
        "caller_class": "TextCleaner",
        "called_function": "__fix_encoding_issues"
      },
      {
        "caller_function": "clean_text",
        "caller_class": "TextCleaner",
        "called_function": "__remove_urls"
      },
      {
        "caller_function": "clean_text",
        "caller_class": "TextCleaner",
        "called_function": "__remove_emails"
      },
      {
        "caller_function": "clean_text",
        "caller_class": "TextCleaner",
        "called_function": "__remove_special_characters"
      },
      {
        "caller_function": "clean_text",
        "caller_class": "TextCleaner",
        "called_function": "__normalize_punctuation"
      },
      {
        "caller_function": "clean_text",
        "caller_class": "TextCleaner",
        "called_function": "__remove_extra_whitespace"
      },
      {
        "caller_function": "clean_text",
        "caller_class": "TextCleaner",
        "called_function": "__lowercase_text"
      },
      {
        "caller_function": "clean_text",
        "caller_class": "TextCleaner",
        "called_function": "__remove_short_lines"
      },
      {
        "caller_function": "get_txt_files",
        "caller_class": "TextCleaner",
        "called_function": "enumerate"
      },
      {
        "caller_function": "get_txt_files",
        "caller_class": "TextCleaner",
        "called_function": "endswith"
      },
      {
        "caller_function": "get_txt_files",
        "caller_class": "TextCleaner",
        "called_function": "lower"
      },
      {
        "caller_function": "get_txt_files",
        "caller_class": "TextCleaner",
        "called_function": "str"
      },
      {
        "caller_function": "get_txt_files",
        "caller_class": "TextCleaner",
        "called_function": "endswith"
      },
      {
        "caller_function": "get_txt_files",
        "caller_class": "TextCleaner",
        "called_function": "lower"
      },
      {
        "caller_function": "get_txt_files",
        "caller_class": "TextCleaner",
        "called_function": "str"
      },
      {
        "caller_function": "get_txt_files",
        "caller_class": "TextCleaner",
        "called_function": "str"
      },
      {
        "caller_function": "get_txt_files",
        "caller_class": "TextCleaner",
        "called_function": "clean_text"
      },
      {
        "caller_function": "get_txt_files",
        "caller_class": "TextCleaner",
        "called_function": "str"
      },
      {
        "caller_function": "get_txt_files_as_list",
        "caller_class": "TextCleaner",
        "called_function": "get_txt_files"
      },
      {
        "caller_function": "get_txt_files_as_list",
        "caller_class": "TextCleaner",
        "called_function": "items"
      },
      {
        "caller_function": "save_cleaned_df",
        "caller_class": "TextCleaner",
        "called_function": "ValueError"
      },
      {
        "caller_function": "save_cleaned_df",
        "caller_class": "TextCleaner",
        "called_function": "copy"
      },
      {
        "caller_function": "save_cleaned_df",
        "caller_class": "TextCleaner",
        "called_function": "range"
      },
      {
        "caller_function": "save_cleaned_df",
        "caller_class": "TextCleaner",
        "called_function": "len"
      },
      {
        "caller_function": "save_cleaned_df",
        "caller_class": "TextCleaner",
        "called_function": "str"
      },
      {
        "caller_function": "save_cleaned_df",
        "caller_class": "TextCleaner",
        "called_function": "endswith"
      },
      {
        "caller_function": "save_cleaned_df",
        "caller_class": "TextCleaner",
        "called_function": "lower"
      },
      {
        "caller_function": "save_cleaned_df",
        "caller_class": "TextCleaner",
        "called_function": "split"
      },
      {
        "caller_function": "save_cleaned_df",
        "caller_class": "TextCleaner",
        "called_function": "lower"
      },
      {
        "caller_function": "save_cleaned_df",
        "caller_class": "TextCleaner",
        "called_function": "str"
      },
      {
        "caller_function": "save_cleaned_df",
        "caller_class": "TextCleaner",
        "called_function": "clean_text"
      }
    ]
  },
  {
    "file": "microservices/transforming_pipeline/components/cleaning_and_transformation/source/text_cleaning_services.py",
    "imports": [
      {
        "module": "os",
        "type": "external"
      },
      {
        "module": "pandas",
        "type": "external"
      },
      {
        "module": "typing",
        "type": "external"
      },
      {
        "module": "AIDocGen.microservices.transforming_pipeline.components.cleaning_and_transformation.source.text_cleaner",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/components/cleaning_and_transformation/tests/test_parquet_creator.py",
    "imports": [
      {
        "module": "pandas",
        "type": "external"
      },
      {
        "module": "os",
        "type": "external"
      },
      {
        "module": "pathlib",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [
      {
        "name": "create_test_parquet",
        "content": "def create_test_parquet(txt_file_path: str, output_parquet_path: str):\n    \"\"\"\n    Create a parquet file from a text file for testing purposes\n    \n    Parameters\n    ----------\n    txt_file_path : str\n        Path to the input text file\n    output_parquet_path : str\n        Path where to save the output parquet file\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(txt_file_path):\n        raise FileNotFoundError(f\"File not found: {txt_file_path}\")\n    \n    # Read the content of the text file\n    with open(txt_file_path, 'r', encoding='utf-8') as f:\n        content = f.read()\n    \n    # Get the filename from the path\n    filename = os.path.basename(txt_file_path)\n    \n    # Create a DataFrame with two columns\n    df = pd.DataFrame({\n        'filename': [filename],\n        'content': [content]\n    })\n    \n    # Save as parquet\n    df.to_parquet(output_parquet_path, index=False)\n    print(f\"Parquet file created successfully: {output_parquet_path}\")\n    print(f\"Shape: {df.shape}\")\n    print(f\"Columns: {list(df.columns)}\")\n    print(f\"File processed: {filename}\")\n    print(f\"Content length: {len(content)} characters\")",
        "signature": {
          "args": [
            "txt_file_path",
            "output_parquet_path"
          ],
          "defaults": []
        },
        "decorators": []
      }
    ],
    "calls": [
      {
        "caller_function": "create_test_parquet",
        "caller_class": null,
        "called_function": "exists"
      },
      {
        "caller_function": "create_test_parquet",
        "caller_class": null,
        "called_function": "FileNotFoundError"
      },
      {
        "caller_function": "create_test_parquet",
        "caller_class": null,
        "called_function": "open"
      },
      {
        "caller_function": "create_test_parquet",
        "caller_class": null,
        "called_function": "read"
      },
      {
        "caller_function": "create_test_parquet",
        "caller_class": null,
        "called_function": "basename"
      },
      {
        "caller_function": "create_test_parquet",
        "caller_class": null,
        "called_function": "DataFrame"
      },
      {
        "caller_function": "create_test_parquet",
        "caller_class": null,
        "called_function": "to_parquet"
      },
      {
        "caller_function": "create_test_parquet",
        "caller_class": null,
        "called_function": "print"
      },
      {
        "caller_function": "create_test_parquet",
        "caller_class": null,
        "called_function": "print"
      },
      {
        "caller_function": "create_test_parquet",
        "caller_class": null,
        "called_function": "print"
      },
      {
        "caller_function": "create_test_parquet",
        "caller_class": null,
        "called_function": "list"
      },
      {
        "caller_function": "create_test_parquet",
        "caller_class": null,
        "called_function": "print"
      },
      {
        "caller_function": "create_test_parquet",
        "caller_class": null,
        "called_function": "print"
      },
      {
        "caller_function": "create_test_parquet",
        "caller_class": null,
        "called_function": "len"
      }
    ]
  },
  {
    "file": "microservices/transforming_pipeline/components/enrichment/README.py",
    "imports": [],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/components/md_to_txt/endpoints/api.py",
    "imports": [
      {
        "module": "fastapi",
        "type": "external"
      },
      {
        "module": "source.schemas",
        "type": "external"
      },
      {
        "module": "source.converter",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/components/md_to_txt/main.py",
    "imports": [
      {
        "module": "uvicorn",
        "type": "external"
      },
      {
        "module": "fastapi",
        "type": "external"
      },
      {
        "module": "fastapi.middleware.cors",
        "type": "external"
      },
      {
        "module": "endpoints.api",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/components/md_to_txt/source/converter.py",
    "imports": [
      {
        "module": "pandas",
        "type": "external"
      },
      {
        "module": "markdown",
        "type": "external"
      },
      {
        "module": "logging",
        "type": "external"
      },
      {
        "module": "docutils.core",
        "type": "external"
      },
      {
        "module": "asyncio",
        "type": "external"
      }
    ],
    "classes": [
      {
        "name": "FileConverter",
        "methods": [
          {
            "name": "__init__",
            "content": "    def __init__(self, input_df: pd.DataFrame, extensions: list = None):\n        self.input_df = input_df\n        self.extensions = extensions",
            "signature": {
              "args": [
                "self",
                "input_df",
                "extensions"
              ],
              "defaults": [
                "None"
              ]
            },
            "decorators": []
          }
        ],
        "decorators": [],
        "inheritances": []
      }
    ],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/components/md_to_txt/source/schemas.py",
    "imports": [
      {
        "module": "pydantic",
        "type": "external"
      },
      {
        "module": "typing",
        "type": "external"
      }
    ],
    "classes": [
      {
        "name": "ParquetConverterResponse",
        "methods": [],
        "decorators": [],
        "inheritances": [
          "BaseModel"
        ]
      },
      {
        "name": "ParquetConverterRequest",
        "methods": [],
        "decorators": [],
        "inheritances": [
          "BaseModel"
        ]
      }
    ],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/components/md_to_txt/tests/test.py",
    "imports": [],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/components/python_to_text_conversion/filtering/main.py",
    "imports": [
      {
        "module": "microservices.transforming_pipeline.components.python_to_text_conversion.filtering.source.cleaner_comments",
        "type": "external"
      },
      {
        "module": "os",
        "type": "external"
      },
      {
        "module": "uvicorn",
        "type": "external"
      },
      {
        "module": "fastapi",
        "type": "external"
      },
      {
        "module": "fastapi.middleware.cors",
        "type": "external"
      },
      {
        "module": "endpoints.filtering_api",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/components/python_to_text_conversion/filtering/source/cleaner_comments.py",
    "imports": [
      {
        "module": "pandas",
        "type": "external"
      },
      {
        "module": "re",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [
      {
        "name": "remove_comments_and_docstrings",
        "content": "def remove_comments_and_docstrings(code: str) -> str:\n    code = re.sub(r'(\"\"\"|\\'\\'\\')(?:.|\\n)*?\\1', '', code)\n    code = re.sub(r'#.*', '', code)\n    return code ",
        "signature": {
          "args": [
            "code"
          ],
          "defaults": []
        },
        "decorators": []
      },
      {
        "name": "cleaner",
        "content": "def cleaner(content: list) -> list:\n    content = content.apply(remove_comments_and_docstrings)\n    return content",
        "signature": {
          "args": [
            "content"
          ],
          "defaults": []
        },
        "decorators": []
      }
    ],
    "calls": [
      {
        "caller_function": "remove_comments_and_docstrings",
        "caller_class": null,
        "called_function": "sub"
      },
      {
        "caller_function": "remove_comments_and_docstrings",
        "caller_class": null,
        "called_function": "sub"
      },
      {
        "caller_function": "cleaner",
        "caller_class": null,
        "called_function": "apply"
      }
    ]
  },
  {
    "file": "microservices/transforming_pipeline/components/python_to_text_conversion/llm_explainer/source/python_explainer.py",
    "imports": [
      {
        "module": "os",
        "type": "external"
      },
      {
        "module": "typing",
        "type": "external"
      },
      {
        "module": "pandas",
        "type": "external"
      },
      {
        "module": "openai",
        "type": "external"
      },
      {
        "module": "dotenv",
        "type": "external"
      },
      {
        "module": "concurrent.futures",
        "type": "external"
      },
      {
        "module": "utils.prompt",
        "type": "external"
      }
    ],
    "classes": [
      {
        "name": "CodeExplainer",
        "methods": [
          {
            "name": "__init__",
            "content": "    def __init__(self, system_prompt: str = SYSTEM_PROMPT, max_workers: int = 16) -> None:\n        load_dotenv()\n        self.client = AzureOpenAI(\n            api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n        )\n        self.model = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n        self.system_prompt = system_prompt\n        self.max_workers = max_workers",
            "signature": {
              "args": [
                "self",
                "system_prompt",
                "max_workers"
              ],
              "defaults": [
                "SYSTEM_PROMPT",
                "16"
              ]
            },
            "decorators": []
          },
          {
            "name": "explain_code",
            "content": "    def explain_code(self, code: str) -> str:\n        \"\"\"\n        Sends a code string to the LLM and retrieves the explanation.\n        \"\"\"\n        response = self.client.chat.completions.create(\n            messages=[\n                {\"role\": \"system\", \"content\": self.system_prompt},\n                {\"role\": \"user\", \"content\": code},\n            ],\n            temperature=0,\n            model=self.model\n        )\n        return response.choices[0].message.content",
            "signature": {
              "args": [
                "self",
                "code"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "_process_row",
            "content": "    def _process_row(self, row: pd.Series) -> str:\n        print(f\"Explaining: {row['name']}\")\n        return self.explain_code(row[\"content\"])",
            "signature": {
              "args": [
                "self",
                "row"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "explain_dataframe",
            "content": "    def explain_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Iterates over a DataFrame with columns 'name' and 'content', replacing\n        'content' with its explanation using parallel processing.\n        \"\"\"\n        explained_contents = []\n        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            futures = {\n                executor.submit(self._process_row, row): idx\n                for idx, row in df.iterrows()\n            }\n            for future in as_completed(futures):\n                explained_contents.append(future.result())\n        return pd.DataFrame({\"name\": df[\"name\"], \"content\": explained_contents})",
            "signature": {
              "args": [
                "self",
                "df"
              ],
              "defaults": []
            },
            "decorators": []
          }
        ],
        "decorators": [],
        "inheritances": []
      }
    ],
    "functions": [],
    "calls": [
      {
        "caller_function": "__init__",
        "caller_class": "CodeExplainer",
        "called_function": "load_dotenv"
      },
      {
        "caller_function": "__init__",
        "caller_class": "CodeExplainer",
        "called_function": "AzureOpenAI"
      },
      {
        "caller_function": "__init__",
        "caller_class": "CodeExplainer",
        "called_function": "getenv"
      },
      {
        "caller_function": "__init__",
        "caller_class": "CodeExplainer",
        "called_function": "getenv"
      },
      {
        "caller_function": "__init__",
        "caller_class": "CodeExplainer",
        "called_function": "getenv"
      },
      {
        "caller_function": "__init__",
        "caller_class": "CodeExplainer",
        "called_function": "getenv"
      },
      {
        "caller_function": "explain_code",
        "caller_class": "CodeExplainer",
        "called_function": "create"
      },
      {
        "caller_function": "_process_row",
        "caller_class": "CodeExplainer",
        "called_function": "print"
      },
      {
        "caller_function": "_process_row",
        "caller_class": "CodeExplainer",
        "called_function": "explain_code"
      },
      {
        "caller_function": "explain_dataframe",
        "caller_class": "CodeExplainer",
        "called_function": "ThreadPoolExecutor"
      },
      {
        "caller_function": "explain_dataframe",
        "caller_class": "CodeExplainer",
        "called_function": "submit"
      },
      {
        "caller_function": "explain_dataframe",
        "caller_class": "CodeExplainer",
        "called_function": "iterrows"
      },
      {
        "caller_function": "explain_dataframe",
        "caller_class": "CodeExplainer",
        "called_function": "as_completed"
      },
      {
        "caller_function": "explain_dataframe",
        "caller_class": "CodeExplainer",
        "called_function": "append"
      },
      {
        "caller_function": "explain_dataframe",
        "caller_class": "CodeExplainer",
        "called_function": "result"
      },
      {
        "caller_function": "explain_dataframe",
        "caller_class": "CodeExplainer",
        "called_function": "DataFrame"
      }
    ]
  },
  {
    "file": "microservices/transforming_pipeline/components/python_to_text_conversion/llm_explainer/source/utils/prompt.py",
    "imports": [],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/components/standardization/README.py",
    "imports": [],
    "classes": [],
    "functions": [],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/main.py",
    "imports": [
      {
        "module": "transforming_pipeline",
        "type": "external"
      }
    ],
    "classes": [],
    "functions": [
      {
        "name": "run_pipeline",
        "content": "def run_pipeline(data, selected_steps):\n    for step in selected_steps:\n        if step in pipeline_steps:\n            data = pipeline_steps[step](data)\n    return data",
        "signature": {
          "args": [
            "data",
            "selected_steps"
          ],
          "defaults": []
        },
        "decorators": []
      }
    ],
    "calls": []
  },
  {
    "file": "microservices/transforming_pipeline/splitter.py",
    "imports": [
      {
        "module": "pandas",
        "type": "external"
      },
      {
        "module": "components.python_to_text_conversion.filtering.source.cleaner_comments",
        "type": "external"
      },
      {
        "module": "components.md_to_txt.source.converter",
        "type": "external"
      },
      {
        "module": "components.cleaning_and_transformation.source.text_cleaner",
        "type": "external"
      },
      {
        "module": "components.python_to_text_conversion.llm_explainer.source.python_explainer",
        "type": "external"
      },
      {
        "module": "asyncio",
        "type": "external"
      },
      {
        "module": "json",
        "type": "external"
      }
    ],
    "classes": [
      {
        "name": "ManageTransforming",
        "methods": [
          {
            "name": "__init__",
            "content": "    def __init__(self, input_path: str, keep_comments: bool):\n        self.input_path, self.keep_comments = input_path, keep_comments\n        self.__read_parquet()\n        self.__run()",
            "signature": {
              "args": [
                "self",
                "input_path",
                "keep_comments"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "__read_parquet",
            "content": "    def __read_parquet(self):\n        self.df = pd.read_parquet(self.input_path)",
            "signature": {
              "args": [
                "self"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "__filter_extension",
            "content": "    def __filter_extension(self, extensions) -> pd.DataFrame:\n        \"\"\"\n        Filter the DataFrame for the specified extensions.\n        \"\"\"\n        mask = self.df['name'].apply(lambda x: any(str(x).endswith(ext) for ext in extensions))\n        return self.df[mask]",
            "signature": {
              "args": [
                "self",
                "extensions"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "__run",
            "content": "    def __run(self):\n        \"\"\"\n        Avvia la pipeline per i tipi di file specificati.\n        \"\"\"\n        for key, value in EXTENSIONS.items():\n            if key == 'file_converter':\n                self.df_files_to_txt = asyncio.run(FileConverter(self.__filter_extension(value), value).file_to_text())\n            elif key == 'python_to_text':\n                self.df_py_to_txt = self.__filter_extension(value)\n                if not self.keep_comments:\n                    self.df_py_to_txt['content'] = cleaner(self.df_py_to_txt['content'])\n                    self.df_py_to_txt = CodeExplainer().explain_dataframe(self.df_py_to_txt)\n            elif key == 'text_cleaner':\n                self.df3 = self.__filter_extension(value)\n                self.df_text_cleaner = TextCleaner(self.df3).save_cleaned_df()\n            else:\n                pass",
            "signature": {
              "args": [
                "self"
              ],
              "defaults": []
            },
            "decorators": []
          },
          {
            "name": "final_df",
            "content": "    def final_df(self) -> pd.DataFrame:\n        return  pd.concat([self.df_files_to_txt, self.df_py_to_txt, self.df_text_cleaner]).reset_index(drop=True)",
            "signature": {
              "args": [
                "self"
              ],
              "defaults": []
            },
            "decorators": []
          }
        ],
        "decorators": [],
        "inheritances": []
      }
    ],
    "functions": [],
    "calls": [
      {
        "caller_function": "__init__",
        "caller_class": "ManageTransforming",
        "called_function": "__read_parquet"
      },
      {
        "caller_function": "__init__",
        "caller_class": "ManageTransforming",
        "called_function": "__run"
      },
      {
        "caller_function": "__read_parquet",
        "caller_class": "ManageTransforming",
        "called_function": "read_parquet"
      },
      {
        "caller_function": "__filter_extension",
        "caller_class": "ManageTransforming",
        "called_function": "apply"
      },
      {
        "caller_function": "__filter_extension",
        "caller_class": "ManageTransforming",
        "called_function": "any"
      },
      {
        "caller_function": "__filter_extension",
        "caller_class": "ManageTransforming",
        "called_function": "endswith"
      },
      {
        "caller_function": "__filter_extension",
        "caller_class": "ManageTransforming",
        "called_function": "str"
      },
      {
        "caller_function": "__run",
        "caller_class": "ManageTransforming",
        "called_function": "items"
      },
      {
        "caller_function": "__run",
        "caller_class": "ManageTransforming",
        "called_function": "run"
      },
      {
        "caller_function": "__run",
        "caller_class": "ManageTransforming",
        "called_function": "file_to_text"
      },
      {
        "caller_function": "__run",
        "caller_class": "ManageTransforming",
        "called_function": "FileConverter"
      },
      {
        "caller_function": "__run",
        "caller_class": "ManageTransforming",
        "called_function": "__filter_extension"
      },
      {
        "caller_function": "__run",
        "caller_class": "ManageTransforming",
        "called_function": "__filter_extension"
      },
      {
        "caller_function": "__run",
        "caller_class": "ManageTransforming",
        "called_function": "cleaner"
      },
      {
        "caller_function": "__run",
        "caller_class": "ManageTransforming",
        "called_function": "explain_dataframe"
      },
      {
        "caller_function": "__run",
        "caller_class": "ManageTransforming",
        "called_function": "CodeExplainer"
      },
      {
        "caller_function": "__run",
        "caller_class": "ManageTransforming",
        "called_function": "__filter_extension"
      },
      {
        "caller_function": "__run",
        "caller_class": "ManageTransforming",
        "called_function": "save_cleaned_df"
      },
      {
        "caller_function": "__run",
        "caller_class": "ManageTransforming",
        "called_function": "TextCleaner"
      },
      {
        "caller_function": "final_df",
        "caller_class": "ManageTransforming",
        "called_function": "reset_index"
      },
      {
        "caller_function": "final_df",
        "caller_class": "ManageTransforming",
        "called_function": "concat"
      }
    ]
  },
  {
    "file": "test2.py",
    "imports": [],
    "classes": [],
    "functions": [],
    "calls": []
  }
]